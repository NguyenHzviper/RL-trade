{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as alpaca\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_point = 'https://paper-api.alpaca.markets'\n",
    "api_key = 'PKIQ2QKAFZ51W05KKKD3'\n",
    "api_secret='0fMdgavxKPkHaYR0l8sJ2gKc3eiHHXKDUMbuYGsT'\n",
    "\n",
    "# Symbols for the cryptocurrencies we'd like to buy.\n",
    "btc = \"BTCUSD\"\n",
    "eth = \"ETHUSD\"\n",
    "\n",
    "api = alpaca.REST(api_key, api_secret, end_point, api_version='v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balance():\n",
    "    account = api.get_account()\n",
    "    return float(account.cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will calculate the number of units one can afford given cash to spend and latest price, and round it down according to order of the precision factor.\n",
    "def calculate_order_size(cash_to_spend, latest_price):\n",
    " precision_factor = 10000\n",
    " units_to_buy = float(cash_to_spend * precision_factor / latest_price)\n",
    " units_to_buy /= precision_factor\n",
    " return units_to_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy(symbol, qty):\n",
    "    api.submit_order(symbol=symbol, qty=qty,side='buy', type='market', time_in_force='gtc')\n",
    "\n",
    "def sell(symbol, qty):\n",
    "    api.submit_order(symbol=symbol, qty=qty,side='sell', type='market', time_in_force='gtc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_price(symbol):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history()\n",
    "    market_price = data['Close'].iloc[-1]\n",
    "    return float(market_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use AAPL (Apple), MSI (Motorola), SBUX (Starbucks)\n",
    "def get_data():\n",
    "  # returns a T x 3 list of stock prices\n",
    "  # each row is a different stock\n",
    "  # 0 = AAPL\n",
    "  # 1 = MSI\n",
    "  # 2 = SBUX\n",
    "  df = pd.read_csv('aapl_msi_sbux.csv')\n",
    "  return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "  # return scikit-learn scaler object to scale the states\n",
    "  # Note: you could also populate the replay buffer here\n",
    "\n",
    "  states = []\n",
    "  for _ in range(env.n_step):\n",
    "    action = np.random.choice(env.action_space)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    states.append(state)\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(states)\n",
    "  return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(agent, env, is_train):\n",
    "  # note: after transforming states are already 1xD\n",
    "  state = env.reset()\n",
    "  state = scaler.transform([state])\n",
    "  done = False\n",
    "\n",
    "  while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = scaler.transform([next_state])\n",
    "    if is_train == 'train':\n",
    "      agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "      agent.replay(batch_size)\n",
    "    state = next_state\n",
    "\n",
    "  return info['cur_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):\n",
    "  \"\"\" A multi-layer perceptron \"\"\"\n",
    "\n",
    "  # input layer\n",
    "  i = Input(shape=(input_dim,))\n",
    "  x = i\n",
    "\n",
    "  # hidden layers\n",
    "  for _ in range(n_hidden_layers):\n",
    "    x = Dense(hidden_dim, activation='relu')(x)\n",
    "\n",
    "  # final layer\n",
    "  x = Dense(n_action)(x)\n",
    "\n",
    "  # make the model\n",
    "  model = Model(i, x)\n",
    "\n",
    "  model.compile(loss='mse', optimizer='adam')\n",
    "  print((model.summary()))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The experience replay memory ###\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, obs_dim, act_dim, size):\n",
    "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "    self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
    "    self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "    self.done_buf = np.zeros(size, dtype=np.uint8)\n",
    "    self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "  def store(self, obs, act, rew, next_obs, done):\n",
    "    self.obs1_buf[self.ptr] = obs\n",
    "    self.obs2_buf[self.ptr] = next_obs\n",
    "    self.acts_buf[self.ptr] = act\n",
    "    self.rews_buf[self.ptr] = rew\n",
    "    self.done_buf[self.ptr] = done\n",
    "    self.ptr = (self.ptr+1) % self.max_size\n",
    "    self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "  def sample_batch(self, batch_size=32):\n",
    "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "    return dict(s=self.obs1_buf[idxs],\n",
    "                s2=self.obs2_buf[idxs],\n",
    "                a=self.acts_buf[idxs],\n",
    "                r=self.rews_buf[idxs],\n",
    "                d=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVM for agent trading on alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvnAlpaca:\n",
    "    \"\"\"\n",
    "    A 3-stock trading environment.\n",
    "    State: vector of size 7 (n_stock * 2 + 1)\n",
    "        - # shares of stock 1 owned\n",
    "        - # shares of stock 2 owned\n",
    "        - # shares of stock 3 owned\n",
    "        - price of stock 1 (using daily close price)\n",
    "        - price of stock 2\n",
    "        - price of stock 3\n",
    "        - cash owned (can be used to purchase more stocks)\n",
    "    Action: categorical variable with 27 (3^3) possibilities\n",
    "        - for each stock, you can:\n",
    "        - 0 = sell\n",
    "        - 1 = hold\n",
    "        - 2 = buy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, symbols, initial_investment=20000):\n",
    "        # data\n",
    "        self.symbols = symbols\n",
    "        self.n_stock = len(symbols)\n",
    "        \n",
    "        # instance attributes\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "\n",
    "        self.action_space = np.arange(3**self.n_stock)\n",
    "\n",
    "        # action permutations\n",
    "        # returns a nested list with elements like:\n",
    "        # [0,0,0]\n",
    "        # [0,0,1]\n",
    "        # [0,0,2]\n",
    "        # [0,1,0]\n",
    "        # [0,1,1]\n",
    "        # etc.\n",
    "        # 0 = sell\n",
    "        # 1 = hold\n",
    "        # 2 = buy\n",
    "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
    "\n",
    "        # calculate size of state\n",
    "        self.state_dim = self.n_stock * 2 + 1\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.stock_owned = np.zeros(self.n_stock)\n",
    "        self.stock_price = self._get_stock_prices()\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "\n",
    "        # get current value before performing the action\n",
    "        prev_val = self._get_val()\n",
    "\n",
    "        # update price, i.e. go to the next day\n",
    "        self.cur_step += 1\n",
    "        self.stock_price = self._get_stock_prices()\n",
    "\n",
    "        # perform the trade\n",
    "        self._trade(action)\n",
    "\n",
    "        # get the new value after taking the action\n",
    "        cur_val = self._get_val()\n",
    "\n",
    "        # reward is the increase in portfolio value\n",
    "        reward = cur_val - prev_val\n",
    "\n",
    "        # done if we have run out of data\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "\n",
    "        # store the current value of the portfolio here\n",
    "        info = {'cur_val': cur_val}\n",
    "\n",
    "        # conform to the Gym API\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_stock] = self.stock_owned\n",
    "        obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
    "        obs[-1] = self.cash_in_hand\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def _get_val(self):\n",
    "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
    "\n",
    "\n",
    "    def _trade(self, action):\n",
    "        # index the action we want to perform\n",
    "        # 0 = sell\n",
    "        # 1 = hold\n",
    "        # 2 = buy\n",
    "        # e.g. [2,1,0] means:\n",
    "        # buy first stock\n",
    "        # hold second stock\n",
    "        # sell third stock\n",
    "        action_vec = self.action_list[action]\n",
    "\n",
    "        # determine which stocks to buy or sell\n",
    "        sell_index = []  # stores index of stocks we want to sell\n",
    "        buy_index = []  # stores index of stocks we want to buy\n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a == 0:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                buy_index.append(i)\n",
    "\n",
    "        # sell any stocks we want to sell\n",
    "        # then buy any stocks we want to buy\n",
    "        if sell_index:\n",
    "            # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
    "            for i in sell_index:\n",
    "                qty = int(self.stock_owned[i])  # Convert float to integer\n",
    "                symbol = self.symbols[i]\n",
    "                sell(symbol, qty)  # Call the sell function\n",
    "                self.stock_owned[i] = 0\n",
    "        if buy_index:\n",
    "            # NOTE: when buying, we will loop through each stock we want to buy,\n",
    "            #       and buy one share at a time until we run out of cash\n",
    "            can_buy = True\n",
    "            while can_buy:\n",
    "                for i in buy_index:\n",
    "                    symbol = self.symbols[i]\n",
    "                    qty = 1\n",
    "                    if self.cash_in_hand > self.stock_price[i]:\n",
    "                        buy(symbol, qty)  # Call the buy function\n",
    "                        self.stock_owned[i] += qty\n",
    "                        self.cash_in_hand -= self.stock_price[i]\n",
    "                    else:\n",
    "                        can_buy = False\n",
    "\n",
    "    def _get_stock_prices(self):\n",
    "        prices = []\n",
    "        for symbol in self.symbols:\n",
    "            market_price = get_latest_price(symbol)\n",
    "            prices.append(market_price)\n",
    "        return np.array(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "    self.gamma = 0.95  # discount rate\n",
    "    self.epsilon = 1.0  # exploration rate\n",
    "    self.epsilon_min = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.model = mlp(state_size, action_size)\n",
    "\n",
    "\n",
    "  def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "    self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return np.random.choice(self.action_size)\n",
    "    act_values = self.model.predict(state)\n",
    "    return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "  def replay(self, batch_size=32):\n",
    "    # first check if replay buffer contains enough data\n",
    "    if self.memory.size < batch_size:\n",
    "      return\n",
    "\n",
    "    # sample a batch of data from the replay memory\n",
    "    minibatch = self.memory.sample_batch(batch_size)\n",
    "    states = minibatch['s']\n",
    "    actions = minibatch['a']\n",
    "    rewards = minibatch['r']\n",
    "    next_states = minibatch['s2']\n",
    "    done = minibatch['d']\n",
    "\n",
    "    # Calculate the tentative target: Q(s',a)\n",
    "    target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "\n",
    "    # With the Keras API, the target (usually) must have the same\n",
    "    # shape as the predictions.\n",
    "    # However, we only need to update the network for the actions\n",
    "    # which were actually taken.\n",
    "    # We can accomplish this by setting the target to be equal to\n",
    "    # the prediction for all values.\n",
    "    # Then, only change the targets for the actions taken.\n",
    "    # Q(s,a)\n",
    "    target_full = self.model.predict(states)\n",
    "    target_full[np.arange(batch_size), actions] = target\n",
    "\n",
    "    # Run one training step\n",
    "    self.model.train_on_batch(states, target_full)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "  def load(self, name):\n",
    "    self.model.load_weights(name)\n",
    "\n",
    "\n",
    "  def save(self, name):\n",
    "    self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                256       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 27)                891       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,147\n",
      "Trainable params: 1,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\env_dlearning\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The following 'Dividends' events are out-of-range, did not expect with interval 1d: DatetimeIndex(['2023-06-14 00:00:00-04:00'], dtype='datetime64[ns, America/New_York]', freq=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\machine_learning_examples\\tf2.0\\test_trader_alpaca.ipynb Cell 21\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m initial_investment \u001b[39m=\u001b[39m get_balance()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m symbols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAAPL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMSI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSBUX\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m env \u001b[39m=\u001b[39m EvnAlpaca(symbols, initial_investment)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m scaler \u001b[39m=\u001b[39m get_scaler(env)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# make sure epsilon is not 1!\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# no need to run multiple episodes if epsilon = 0, it's deterministic\u001b[39;00m\n",
      "\u001b[1;32md:\\machine_learning_examples\\tf2.0\\test_trader_alpaca.ipynb Cell 21\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# calculate size of state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_stock \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n",
      "\u001b[1;32md:\\machine_learning_examples\\tf2.0\\test_trader_alpaca.ipynb Cell 21\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstock_owned \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_stock)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstock_price \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_stock_prices()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcash_in_hand \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_investment\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs()\n",
      "\u001b[1;32md:\\machine_learning_examples\\tf2.0\\test_trader_alpaca.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m prices \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msymbols:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     market_price \u001b[39m=\u001b[39m get_latest_price(symbol)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     prices\u001b[39m.\u001b[39mappend(market_price)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(prices)\n",
      "\u001b[1;32md:\\machine_learning_examples\\tf2.0\\test_trader_alpaca.ipynb Cell 21\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_latest_price\u001b[39m(symbol):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ticker \u001b[39m=\u001b[39m yf\u001b[39m.\u001b[39mTicker(symbol)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data \u001b[39m=\u001b[39m ticker\u001b[39m.\u001b[39;49mhistory()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     market_price \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/machine_learning_examples/tf2.0/test_trader_alpaca.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m(market_price)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\env_dlearning\\lib\\site-packages\\yfinance\\utils.py:105\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEntering \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m IndentationContext():\n\u001b[1;32m--> 105\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExiting \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\env_dlearning\\lib\\site-packages\\yfinance\\base.py:383\u001b[0m, in \u001b[0;36mTickerBase.history\u001b[1;34m(self, period, interval, start, end, prepost, actions, auto_adjust, back_adjust, repair, keepna, proxy, rounding, timeout, debug, raise_errors)\u001b[0m\n\u001b[0;32m    381\u001b[0m df \u001b[39m=\u001b[39m quotes\u001b[39m.\u001b[39msort_index()\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m dividends\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 383\u001b[0m     df \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49msafe_merge_dfs(df, dividends, interval)\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDividends\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m    385\u001b[0m     df\u001b[39m.\u001b[39mloc[df[\u001b[39m\"\u001b[39m\u001b[39mDividends\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misna(), \u001b[39m\"\u001b[39m\u001b[39mDividends\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\env_dlearning\\lib\\site-packages\\yfinance\\utils.py:721\u001b[0m, in \u001b[0;36msafe_merge_dfs\u001b[1;34m(df_main, df_sub, interval)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mif\u001b[39;00m f_outOfRange\u001b[39m.\u001b[39many():\n\u001b[0;32m    720\u001b[0m     \u001b[39mif\u001b[39;00m intraday \u001b[39mor\u001b[39;00m interval \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m1d\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1wk\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m--> 721\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdata_col\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m events are out-of-range, did not expect with interval \u001b[39m\u001b[39m{\u001b[39;00minterval\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdf_sub\u001b[39m.\u001b[39mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    722\u001b[0m     get_yf_logger()\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDiscarding these \u001b[39m\u001b[39m{\u001b[39;00mdata_col\u001b[39m}\u001b[39;00m\u001b[39m events:\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(df_sub[f_outOfRange]))\n\u001b[0;32m    723\u001b[0m     df_sub \u001b[39m=\u001b[39m df_sub[\u001b[39m~\u001b[39mf_outOfRange]\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;31mException\u001b[0m: The following 'Dividends' events are out-of-range, did not expect with interval 1d: DatetimeIndex(['2023-06-14 00:00:00-04:00'], dtype='datetime64[ns, America/New_York]', freq=None)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "  # config\n",
    "  models_folder = 'rl_trader_models'\n",
    "  rewards_folder = 'rl_trader_rewards'\n",
    "  num_episodes = 25\n",
    "  batch_size = 32\n",
    "\n",
    "  args = \"test\"\n",
    "\n",
    "  maybe_make_dir(models_folder)\n",
    "  maybe_make_dir(rewards_folder)\n",
    "\n",
    "  data = get_data()\n",
    "  n_timesteps, n_stocks = data.shape\n",
    "\n",
    "  n_train = n_timesteps // 2\n",
    "\n",
    "  train_data = data[:n_train]\n",
    "  test_data = data[n_train:]\n",
    "\n",
    "  state_size = env.state_dim\n",
    "  action_size = len(env.action_space)\n",
    "  agent = DQNAgent(state_size, action_size)\n",
    "  \n",
    "\n",
    "  # store the final value of the portfolio (end of episode)\n",
    "  portfolio_value = []\n",
    "\n",
    "  if args == 'test':\n",
    "    # then load the previous scaler\n",
    "    with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
    "      scaler = pickle.load(f)\n",
    "\n",
    "    # remake the env with test data\n",
    "    initial_investment = get_balance()\n",
    "    symbols = ['AAPL', 'MSI', 'SBUX']\n",
    "    env = EvnAlpaca(symbols, initial_investment)\n",
    "    scaler = get_scaler(env)\n",
    "\n",
    "    # make sure epsilon is not 1!\n",
    "    # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    # load trained weights\n",
    "    agent.load(f'{models_folder}/dqn.h5')\n",
    "\n",
    "  # play the game num_episodes times\n",
    "  for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, args)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\n",
    "    portfolio_value.append(val) # append episode end portfolio value\n",
    "\n",
    "  # save portfolio value for each episode\n",
    "  np.save(f'{rewards_folder}/{args}.npy', portfolio_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
